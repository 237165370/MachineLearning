然而 ， 即使 上述 模型 对词 向量 进行 平均 处理 ， 我们 仍然 忽略 了 单词 之间 的 排列 顺序 对 情感 分析 的 影响 。 即 上述 的 word2vec 只是 基于 词 的 维度 进行 " 语义 分析 " 的 ， 而 并 不 具有 上下文 的 " 语义 分析 " 能力 。 作为 一个 处理 可变 长度 文本 的 总结性 方法 ， QuocLe 和 TomasMikolov 提出 了 Doc2Vec 方法 。 除了 增加 一个 段落 向量 以外 ， 这个 方法 几乎 等同于 Word2Vec 。 和 Word2Vec 一样 ， 该 模型 也 存在 两种 方法 ： DistributedMemory ( DM ) 和 DistributedBagofWords ( DBOW ) 。 DM 试图 在 给定 上下文 和 段落 向量 的 情况 下 预测 单词 的 概率 。 在 一个 句子 或者 文档 的 训练 过程 中 ， 段落 ID 保持 不变 ， 共享 着 同一个 段落 向量 。 DBOW 则 在 仅 给定 段落 向量 的 情况 下 预测 段落 中 一组 随机 单词 的 概率 。 然而 ， 即使 上述 模型 对词 向量 进行 平均 处理 ， 我们 仍然 忽略 了 单词 之间 的 排列 顺序 对 情感 分析 的 影响 。 即 上述 的 word2vec 只是 基于 词 的 维度 进行 " 语义 分析 " 的 ， 而 并 不 具有 上下文 的 " 语义 分析 " 能力 。 作为 一个 处理 可变 长度 文本 的 总结性 方法 ， QuocLe 和 TomasMikolov 提出 了 Doc2Vec 方法 。 除了 增加 一个 段落 向量 以外 ， 这个 方法 几乎 等同于 Word2Vec 。 和 Word2Vec 一样 ， 该 模型 也 存在 两种 方法 ： DistributedMemory ( DM ) 和 DistributedBagofWords ( DBOW ) 。 DM 试图 在 给定 上下文 和 段落 向量 的 情况 下 预测 单词 的 概率 。 在 一个 句子 或者 文档 的 训练 过程 中 ， 段落 ID 保持 不变 ， 共享 着 同一个 段落 向量 。 DBOW 则 在 仅 给定 段落 向量 的 情况 下 预测 段落 中 一组 随机 单词 的 概率 。 